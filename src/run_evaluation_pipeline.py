#!/usr/bin/env python3
"""
SCRIPT MAESTRO - Pipeline Completo de Evaluaci√≥n
Self-Driving Car using Reinforcement Learning

Ejecuta evaluaci√≥n de todos los modelos y genera reportes comparativos.
"""

import sys
import os
from pathlib import Path

# Importar evaluadores
from evaluate_pro import RobustEvaluator
from compare_models import ComparativeAnalysis


def print_banner(title):
    """Imprime un banner formateado."""
    width = 80
    print("\n" + "=" * width)
    print(f"{title.center(width)}")
    print("=" * width + "\n")


def main():
    """Ejecuta el pipeline completo de evaluaci√≥n."""
    
    print_banner("PIPELINE DE EVALUACI√ìN - SELF-DRIVING CAR RL")
    
    # ========== CONFIGURACI√ìN DE MODELOS ==========
    # CAMBIAR ESTAS RUTAS A TUS MODELOS REALES
    
    models_to_evaluate = {
        'ppo_car_racing_step_500000': r'C:\Users\emped\OneDrive\Documentos\MIS COSAS\Yo\3 CURSO\Selfdriving-car_RL-\Models\models_T3\ppo_car_racing_step_491520.pth',
        'ppo_car_racing_step_1000000': r'C:\Users\emped\OneDrive\Documentos\MIS COSAS\Yo\3 CURSO\Selfdriving-car_RL-\Models\models_T3\ppo_car_racing_step_1064960.pth',
        'ppo_car_racing_step_2000000': r'C:\Users\emped\OneDrive\Documentos\MIS COSAS\Yo\3 CURSO\Selfdriving-car_RL-\Models\models_T3\ppo_car_racing_final.pth',
    }
    
    # ========== PAR√ÅMETROS DE EVALUACI√ìN ==========
    num_episodes = 30  # Episodios por modelo
    seed = 100         # Seed para reproducibilidad
    
    # ========== FASE 1: EVALUAR CADA MODELO ==========
    
    print_banner("FASE 1: Evaluaci√≥n de Modelos Individuales")
    
    evaluation_results = {}
    
    for model_name, model_path in models_to_evaluate.items():
        print(f"\nüìä Evaluando: {model_name}")
        print(f"   Archivo: {model_path}")
        print("-" * 80)
        
        # Verificar que el archivo existe
        if not Path(model_path).exists():
            print(f"‚ö†Ô∏è  ADVERTENCIA: No se encontr√≥ el archivo {model_path}")
            print(f"   Por favor, actualiza la ruta en el script.")
            continue
        
        try:
            # Crear evaluador
            evaluator = RobustEvaluator(
                model_path=model_path,
                num_episodes=num_episodes,
                seed=seed
            )
            
            # Ejecutar evaluaci√≥n
            all_metrics, stats = evaluator.run()
            
            evaluation_results[model_name] = {
                'metrics': all_metrics,
                'stats': stats
            }
            
            print(f"\n‚úÖ {model_name} evaluado exitosamente")
            
        except Exception as e:
            print(f"\n‚ùå Error evaluando {model_name}:")
            print(f"   {str(e)}")
            continue
    
    if not evaluation_results:
        print("\n" + "!" * 80)
        print("ERROR: No se pudo evaluar ning√∫n modelo.")
        print("Por favor, verifica las rutas de los modelos en la configuraci√≥n.")
        print("!" * 80)
        return False
    
    # ========== FASE 2: AN√ÅLISIS COMPARATIVO ==========
    
    print_banner("FASE 2: An√°lisis Comparativo")
    
    try:
        # Crear analizador
        analyzer = ComparativeAnalysis(evaluation_results_dir="evaluation_results")
        
        # Cargar modelos evaluados
        model_names = list(evaluation_results.keys())
        
        # Ejecutar an√°lisis completo
        report = analyzer.run_full_comparison(model_names)
        
        print(report)
        
    except Exception as e:
        print(f"\n‚ùå Error en an√°lisis comparativo:")
        print(f"   {str(e)}")
        return False
    
    # ========== RESUMEN FINAL ==========
    
    print_banner("‚úÖ PIPELINE COMPLETADO")
    
    print("\nüìÅ ARCHIVOS GENERADOS:\n")
    
    print("  RESULTADOS INDIVIDUALES:")
    print("  ‚îî‚îÄ evaluation_results/")
    for model_name in evaluation_results.keys():
        print(f"      ‚îú‚îÄ {model_name}/")
        print(f"      ‚îÇ   ‚îú‚îÄ results.json          (Datos detallados)")
        print(f"      ‚îÇ   ‚îú‚îÄ evaluation_plots.png  (Gr√°ficos)")
        print(f"      ‚îÇ   ‚îú‚îÄ report.txt            (Reporte textual)")
        print(f"      ‚îÇ   ‚îî‚îÄ videos/               (Videos de episodios)")
    
    print("\n  AN√ÅLISIS COMPARATIVO:")
    print("  ‚îî‚îÄ comparison_analysis/")
    print("      ‚îú‚îÄ model_comparison.png         (Gr√°ficos comparativos)")
    print("      ‚îú‚îÄ reward_distributions.png     (Distribuciones)")
    print("      ‚îú‚îÄ comparison_report.txt        (Reporte comparativo)")
    print("      ‚îî‚îÄ model_comparison.csv         (Datos en CSV)")
    
    print("\n" + "=" * 80)
    print("üìä PR√ìXIMOS PASOS PARA TU PRESENTACI√ìN:")
    print("=" * 80)
    print("""
1. DOCUMENTACI√ìN:
   - Copia comparison_report.txt a tu documentaci√≥n
   - Incluye las gr√°ficas (model_comparison.png, reward_distributions.png)
   - Genera un PDF con los resultados

2. PRESENTACI√ìN:
   - Usa los videos de evaluation_results/*/videos/ en tu presentaci√≥n
   - Incluye los gr√°ficos de comparativa
   - Prepara un resumen de hallazgos principales

3. AN√ÅLISIS:
   - Identifica qu√© modelo tiene mejor rendimiento
   - Analiza la estabilidad (desviaci√≥n est√°ndar)
   - Documenta lecciones aprendidas

4. MEJORAS FUTURAS:
   - Considera entrenamiento adicional si no alcanzas 900
   - Ajusta hyperpar√°metros basado en los resultados
   - Experimenta con diferentes seeds para robustez
    """)
    
    print("=" * 80 + "\n")
    
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
